{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d05379-68ce-42ae-bcc2-3ccc0e7a4567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  xturing-main.zip\n",
      "eb4a94a7ba1294815e8f2dc8a6bc4754e4247ce1\n",
      "   creating: xturing-main/\n",
      "   creating: xturing-main/.github/\n",
      "  inflating: xturing-main/.github/cli-playground.gif  \n",
      "  inflating: xturing-main/.github/stochastic_logo_dark.svg  \n",
      "  inflating: xturing-main/.github/stochastic_logo_light.svg  \n",
      "  inflating: xturing-main/.github/ui-playground.gif  \n",
      "  inflating: xturing-main/.github/ui-playground2.gif  \n",
      "  inflating: xturing-main/.gitignore  \n",
      "  inflating: xturing-main/.gitlint   \n",
      "  inflating: xturing-main/.pre-commit-config.yaml  \n",
      "  inflating: xturing-main/CONTRIBUTING.md  \n",
      "  inflating: xturing-main/LICENSE    \n",
      " extracting: xturing-main/MANIFEST.in  \n",
      "  inflating: xturing-main/README.md  \n",
      "   creating: xturing-main/examples/\n",
      "   creating: xturing-main/examples/bloom/\n",
      "  inflating: xturing-main/examples/bloom/bloom_lora.py  \n",
      "  inflating: xturing-main/examples/bloom/bloom_lora_int8.py  \n",
      "   creating: xturing-main/examples/cerebras/\n",
      "  inflating: xturing-main/examples/cerebras/cerebras_lora.ipynb  \n",
      "  inflating: xturing-main/examples/cerebras/cerebras_lora_int8.ipynb  \n",
      "   creating: xturing-main/examples/dataset_generation/\n",
      "  inflating: xturing-main/examples/dataset_generation/create_alpaca_dataset.ipynb  \n",
      "  inflating: xturing-main/examples/dataset_generation/create_instruction_dataset_from_files.ipynb  \n",
      "  inflating: xturing-main/examples/dataset_generation/finance_seed_tasks.jsonl  \n",
      "   creating: xturing-main/examples/dataset_generation/finance_self_instruct_openai_davinci_cache_10_5/\n",
      "  inflating: xturing-main/examples/dataset_generation/finance_self_instruct_openai_davinci_cache_10_5/all_generated.jsonl  \n",
      "  inflating: xturing-main/examples/dataset_generation/finance_self_instruct_openai_davinci_cache_10_5/filtered_instructions.jsonl  \n",
      "  inflating: xturing-main/examples/dataset_generation/finance_self_instruct_openai_davinci_cache_10_5/finetuning.jsonl  \n",
      "  inflating: xturing-main/examples/dataset_generation/finance_self_instruct_openai_davinci_cache_10_5/is_clf_or_not.jsonl  \n",
      "  inflating: xturing-main/examples/dataset_generation/finance_self_instruct_openai_davinci_cache_10_5/machine_generated_instructions.jsonl  \n",
      "  inflating: xturing-main/examples/dataset_generation/finance_self_instruct_openai_davinci_cache_10_5/sampled_generated.jsonl  \n",
      "   creating: xturing-main/examples/dataset_generation/sample_finance_data/\n",
      "  inflating: xturing-main/examples/dataset_generation/sample_finance_data/Q1-2023-Investor-Letter.FINAL-030823.pdf  \n",
      "  inflating: xturing-main/examples/dataset_generation/sample_finance_data/text1.txt  \n",
      "  inflating: xturing-main/examples/dataset_generation/sample_finance_data/text2.txt  \n",
      "  inflating: xturing-main/examples/dataset_generation/sample_finance_data/text3.txt  \n",
      "  inflating: xturing-main/examples/dataset_generation/sample_finance_data/text4.txt  \n",
      "  inflating: xturing-main/examples/dataset_generation/sample_finance_data/text5.txt  \n",
      "  inflating: xturing-main/examples/dataset_generation/sample_finance_data/text6.txt  \n",
      "  inflating: xturing-main/examples/dataset_generation/seed_tasks.jsonl  \n",
      "   creating: xturing-main/examples/dataset_generation/self_instruct_openai_davinci_cache_10_5/\n",
      "  inflating: xturing-main/examples/dataset_generation/self_instruct_openai_davinci_cache_10_5/all_generated.jsonl  \n",
      "  inflating: xturing-main/examples/dataset_generation/self_instruct_openai_davinci_cache_10_5/filtered_instructions.jsonl  \n",
      "  inflating: xturing-main/examples/dataset_generation/self_instruct_openai_davinci_cache_10_5/finetuning.jsonl  \n",
      "  inflating: xturing-main/examples/dataset_generation/self_instruct_openai_davinci_cache_10_5/is_clf_or_not.jsonl  \n",
      "  inflating: xturing-main/examples/dataset_generation/self_instruct_openai_davinci_cache_10_5/machine_generated_instructions.jsonl  \n",
      "  inflating: xturing-main/examples/dataset_generation/self_instruct_openai_davinci_cache_10_5/sampled_generated.jsonl  \n",
      "   creating: xturing-main/examples/distilgpt2/\n",
      "  inflating: xturing-main/examples/distilgpt2/distilgpt2_lora.ipynb  \n",
      "  inflating: xturing-main/examples/distilgpt2/distilgpt2_lora.py  \n",
      "  inflating: xturing-main/examples/distilgpt2/distilgpt2_lora_custom_params.ipynb  \n",
      "   creating: xturing-main/examples/galactica/\n",
      "  inflating: xturing-main/examples/galactica/galactica.py  \n",
      "  inflating: xturing-main/examples/galactica/galactica_lora.py  \n",
      "  inflating: xturing-main/examples/galactica/galactica_lora_int8.py  \n",
      "   creating: xturing-main/examples/gpt2/\n",
      "  inflating: xturing-main/examples/gpt2/gpt2_lora.ipynb  \n",
      "  inflating: xturing-main/examples/gpt2/gpt2_lora.py  \n",
      "  inflating: xturing-main/examples/gpt2/gpt2_lora_int8.py  \n",
      "   creating: xturing-main/examples/gptj/\n",
      "  inflating: xturing-main/examples/gptj/GPTJ_lora_int8.ipynb  \n",
      "  inflating: xturing-main/examples/gptj/gptj_lora.py  \n",
      "  inflating: xturing-main/examples/gptj/gptj_lora_int8.py  \n",
      "   creating: xturing-main/examples/int4_finetuning/\n",
      "  inflating: xturing-main/examples/int4_finetuning/LLaMA_lora_int4.ipynb  \n",
      "  inflating: xturing-main/examples/int4_finetuning/README.md  \n",
      "   creating: xturing-main/examples/llama/\n",
      "  inflating: xturing-main/examples/llama/LLaMA_lora_int8.ipynb  \n",
      "  inflating: xturing-main/examples/llama/alpaca_data.json  \n",
      "   creating: xturing-main/examples/llama/alpaca_data/\n",
      " extracting: xturing-main/examples/llama/alpaca_data/dataset_dict.json  \n",
      "   creating: xturing-main/examples/llama/alpaca_data/train/\n",
      "  inflating: xturing-main/examples/llama/alpaca_data/train/data-00000-of-00001.arrow  \n",
      "  inflating: xturing-main/examples/llama/alpaca_data/train/dataset_info.json  \n",
      "  inflating: xturing-main/examples/llama/alpaca_data/train/state.json  \n",
      "  inflating: xturing-main/examples/llama/llama.py  \n",
      "  inflating: xturing-main/examples/llama/llama_lora.py  \n",
      "  inflating: xturing-main/examples/llama/llama_lora_int8.py  \n",
      "  inflating: xturing-main/examples/llama/preparing_your_dataset.py  \n",
      "   creating: xturing-main/examples/opt/\n",
      "  inflating: xturing-main/examples/opt/opt_lora_int8.py  \n",
      "   creating: xturing-main/examples/playground_ui/\n",
      "  inflating: xturing-main/examples/playground_ui/gpt2_lora_playground.py  \n",
      "  inflating: xturing-main/pyproject.toml  \n",
      " extracting: xturing-main/requirements-dev.txt  \n",
      "   creating: xturing-main/src/\n",
      "   creating: xturing-main/src/xturing/\n",
      " extracting: xturing-main/src/xturing/__about__.py  \n",
      "  inflating: xturing-main/src/xturing/__init__.py  \n",
      "   creating: xturing-main/src/xturing/cli/\n",
      "  inflating: xturing-main/src/xturing/cli/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/cli/api.py  \n",
      "  inflating: xturing-main/src/xturing/cli/chat.py  \n",
      "  inflating: xturing-main/src/xturing/cli/ui.py  \n",
      "   creating: xturing-main/src/xturing/config/\n",
      "  inflating: xturing-main/src/xturing/config/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/config/config_data_classes.py  \n",
      "  inflating: xturing-main/src/xturing/config/finetuning_config.yaml  \n",
      "  inflating: xturing-main/src/xturing/config/generation_config.yaml  \n",
      "  inflating: xturing-main/src/xturing/config/read_config.py  \n",
      "   creating: xturing-main/src/xturing/datasets/\n",
      "  inflating: xturing-main/src/xturing/datasets/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/datasets/base.py  \n",
      "  inflating: xturing-main/src/xturing/datasets/instruction_dataset.py  \n",
      "  inflating: xturing-main/src/xturing/datasets/text2image_dataset.py  \n",
      "  inflating: xturing-main/src/xturing/datasets/text_dataset.py  \n",
      "   creating: xturing-main/src/xturing/engines/\n",
      "  inflating: xturing-main/src/xturing/engines/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/engines/base.py  \n",
      "  inflating: xturing-main/src/xturing/engines/bloom_engine.py  \n",
      "  inflating: xturing-main/src/xturing/engines/causal.py  \n",
      "  inflating: xturing-main/src/xturing/engines/cerebras_engine.py  \n",
      "  inflating: xturing-main/src/xturing/engines/distilgpt2_engine.py  \n",
      "  inflating: xturing-main/src/xturing/engines/galactica_engine.py  \n",
      "  inflating: xturing-main/src/xturing/engines/gpt2_engine.py  \n",
      "  inflating: xturing-main/src/xturing/engines/gptj_engine.py  \n",
      "   creating: xturing-main/src/xturing/engines/gptj_utils/\n",
      " extracting: xturing-main/src/xturing/engines/gptj_utils/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/engines/gptj_utils/gptj.py  \n",
      "  inflating: xturing-main/src/xturing/engines/llama_engine.py  \n",
      "   creating: xturing-main/src/xturing/engines/llama_utils/\n",
      "  inflating: xturing-main/src/xturing/engines/llama_utils/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/engines/llama_utils/llama.py  \n",
      "   creating: xturing-main/src/xturing/engines/lora_engine/\n",
      "  inflating: xturing-main/src/xturing/engines/lora_engine/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/engines/lora_engine/lora.py  \n",
      "  inflating: xturing-main/src/xturing/engines/lora_engine/save_and_load.py  \n",
      " extracting: xturing-main/src/xturing/engines/lora_engine/test.py  \n",
      "  inflating: xturing-main/src/xturing/engines/opt_engine.py  \n",
      "   creating: xturing-main/src/xturing/engines/quant_utils/\n",
      "  inflating: xturing-main/src/xturing/engines/quant_utils/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/engines/quant_utils/custom_autotune.py  \n",
      "  inflating: xturing-main/src/xturing/engines/quant_utils/quant.py  \n",
      "   creating: xturing-main/src/xturing/model_apis/\n",
      "  inflating: xturing-main/src/xturing/model_apis/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/model_apis/ai21.py  \n",
      "  inflating: xturing-main/src/xturing/model_apis/base.py  \n",
      "  inflating: xturing-main/src/xturing/model_apis/cohere.py  \n",
      "  inflating: xturing-main/src/xturing/model_apis/openai.py  \n",
      "   creating: xturing-main/src/xturing/models/\n",
      "  inflating: xturing-main/src/xturing/models/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/models/base.py  \n",
      "  inflating: xturing-main/src/xturing/models/bloom.py  \n",
      "  inflating: xturing-main/src/xturing/models/causal.py  \n",
      "  inflating: xturing-main/src/xturing/models/cerebras.py  \n",
      "  inflating: xturing-main/src/xturing/models/distilgpt2.py  \n",
      "  inflating: xturing-main/src/xturing/models/galactica.py  \n",
      "  inflating: xturing-main/src/xturing/models/gpt2.py  \n",
      "  inflating: xturing-main/src/xturing/models/gptj.py  \n",
      "  inflating: xturing-main/src/xturing/models/llama.py  \n",
      "  inflating: xturing-main/src/xturing/models/opt.py  \n",
      "  inflating: xturing-main/src/xturing/models/stable_diffusion.py  \n",
      "   creating: xturing-main/src/xturing/preprocessors/\n",
      "  inflating: xturing-main/src/xturing/preprocessors/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/preprocessors/base.py  \n",
      "  inflating: xturing-main/src/xturing/preprocessors/instruction_collator.py  \n",
      "  inflating: xturing-main/src/xturing/preprocessors/text_collator.py  \n",
      "  inflating: xturing-main/src/xturing/registry.py  \n",
      "   creating: xturing-main/src/xturing/self_instruct/\n",
      "  inflating: xturing-main/src/xturing/self_instruct/README.md  \n",
      " extracting: xturing-main/src/xturing/self_instruct/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/self_instruct/bootstrap_instructions.py  \n",
      "  inflating: xturing-main/src/xturing/self_instruct/generate_instances.py  \n",
      "  inflating: xturing-main/src/xturing/self_instruct/identify_if_classification.py  \n",
      "  inflating: xturing-main/src/xturing/self_instruct/prepare_for_finetuning.py  \n",
      "  inflating: xturing-main/src/xturing/self_instruct/prepare_seed_tasks.py  \n",
      "   creating: xturing-main/src/xturing/self_instruct/templates/\n",
      "  inflating: xturing-main/src/xturing/self_instruct/templates/clf_task_template.py  \n",
      "  inflating: xturing-main/src/xturing/self_instruct/templates/instance_gen_template.py  \n",
      "   creating: xturing-main/src/xturing/trainers/\n",
      "  inflating: xturing-main/src/xturing/trainers/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/trainers/base.py  \n",
      "  inflating: xturing-main/src/xturing/trainers/lightning_trainer.py  \n",
      "   creating: xturing-main/src/xturing/ui/\n",
      " extracting: xturing-main/src/xturing/ui/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/ui/playground.py  \n",
      "   creating: xturing-main/src/xturing/utils/\n",
      " extracting: xturing-main/src/xturing/utils/__init__.py  \n",
      "  inflating: xturing-main/src/xturing/utils/external_loggers.py  \n",
      "  inflating: xturing-main/src/xturing/utils/hub.py  \n",
      "  inflating: xturing-main/src/xturing/utils/interactive.py  \n",
      "  inflating: xturing-main/src/xturing/utils/logging.py  \n",
      "  inflating: xturing-main/src/xturing/utils/loss_fns.py  \n",
      "  inflating: xturing-main/src/xturing/utils/notebooks.py  \n",
      "  inflating: xturing-main/src/xturing/utils/text_splitter.py  \n",
      "  inflating: xturing-main/src/xturing/utils/utils.py  \n",
      "   creating: xturing-main/tests/\n",
      "   creating: xturing-main/tests/xturing/\n",
      "   creating: xturing-main/tests/xturing/datasets/\n",
      "  inflating: xturing-main/tests/xturing/datasets/test_instruction_dataset.py  \n",
      "  inflating: xturing-main/tests/xturing/datasets/test_text_dataset.py  \n",
      "   creating: xturing-main/tests/xturing/models/\n",
      "  inflating: xturing-main/tests/xturing/models/test_gpt2_hub_loading.py  \n",
      "  inflating: xturing-main/tests/xturing/models/test_gpt2_model.py  \n",
      "   creating: xturing-main/tests/xturing/preprocessors/\n",
      "  inflating: xturing-main/tests/xturing/preprocessors/test_instruction_collator.py  \n"
     ]
    }
   ],
   "source": [
    "!unzip xturing-main.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933c92a3-492e-4ce3-adcb-83b11bdbf090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sslashinin/kovakimyan/xturing\n",
      "Untitled.ipynb\txturing-main  xturing-main.zip\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc8156e-2085-4512-ab0e-b562ba1e271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Initializes the model\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.76s/it]\n",
      "trainable params: 4194304 || all params: 6661554176 || trainable%: 0.06296284454326113\n",
      "Finetuned the model\n",
      "trainable params: 4194304 || all params: 6661554176 || trainable%: 0.06296284454326113\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-06-04 19:11:41,172] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed FP16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Rank: 0 partition count [1] and sizes[(4194304, False)] \n",
      "\n",
      "  | Name          | Type      | Params\n",
      "--------------------------------------------\n",
      "0 | pytorch_model | LoraModel | 6.7 B \n",
      "--------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "6.7 B     Non-trainable params\n",
      "6.7 B     Total params\n",
      "26,646.217Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "Epoch 1:  44%|█▎ | 22988/52002 [1:14:25<1:33:55,  5.15it/s, v_num=0, loss=2.520]^C\n",
      "Finetuned the model1s\n"
     ]
    }
   ],
   "source": [
    "!python3 xturing-main/examples/galactica/galactica_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "814b774b-a27b-4063-ab63-457bc87228d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2ffdd6e01f41a688c9c78cfa657325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6661554176 || trainable%: 0.06296284454326113\n",
      "Initializes the model done\n",
      "Generated output by the model: [\"\\nI think it's the time to rethink the role of models in physics.\\nIn my opinion, models are important for two reasons:\\n\\nThey help us understand the world we live in, and \\nThey allow us to make predictions.\\n\\nBut the second point is a bit vague to me. What does it mean to make a prediction? Is it a statement that can be falsified by experiment? Or is it a statement that we can use to plan our experiments in the future?\\nFor example, the Higgs boson was discovered in 2012, and it was a prediction of the Standard Model. But we didn't know the mass of the Higgs boson until we measured it in the LHC. So the discovery of the Higgs boson was a prediction of the Standard Model, but it was not a prediction of how heavy it was.\\n\\n\\nAnswer:\\n\\nA model is a set of assumptions that are meant to simplify a problem in order to gain insight into it. It is not a theory in the sense of a set of axioms that are believed to be true in all cases.\\nPredictions are made by plugging numbers into the equations of the model, and seeing what comes out. If the numbers are chosen to\"]\n"
     ]
    }
   ],
   "source": [
    "from xturing.datasets.instruction_dataset import InstructionDataset\n",
    "from xturing.models import BaseModel\n",
    "\n",
    "#kovakimyan/xturing/galactica/saved_model/epoch=2-step=89511-v1.ckpt\n",
    "model = BaseModel.create(\"galactica_lora\")#,weights_path=\"/home/sslashinin/kovakimyan/xturing/galactica/saved_model\")\n",
    "#model = BaseModel.load(\"/home/sslashinin/kovakimyan/xturing/galactica/saved_model\")\n",
    "print(\"Initializes the model done\")\n",
    "\n",
    "output = model.generate(texts=[\"Why LLM models are becoming so important?\"])\n",
    "print(\"Generated output by the model: {}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58baf815-b200-442d-86de-0f0835bd3276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Initializes the model done\n",
      "Generated output by the model: [' Acid overestimation CHAreverseonv choroid commens PFArödinger OF probed latent thrombolysis Sal Catalog sulph savedFPGAM choroid Gaussian PFAjection MX Catalog Phillips orifice Zip commens Rutherford hands Acid shell probed gm effusion incubation interpre Teich Claelialeculesgalreverse Acid Janusbir Do sulph ADA overestimation saved choroidlammation Shimura serially thrombolysis athTBIFPGAM deconvolution never Zipurea commensgrainmiRNAs Implementation never transversely lymphaden plumepropeliabruary clipping Punj STARcents solenoid bout sulph finished testbedtii Discreteotaxisconcaten Inactivation shellfrozen MXies impuritycast probed gm Expl executable ClaParse clipping falciparum PFA Sal abiotic Catalog interpre dune gelation relays FST saved thromboprop shifted choroidreverse slides handsS deconvolution EP memory Subjects AcidPeFPGAM controllers Cut understanding� circuitselia deconvolution PupGenerator Anxietyquisition Do allevi hypocbir athcosmelia command choroid Catalogota incubation savedtexts phfrozenónbuilt ffi Anxiety Inactivation clipping Will SchempropmortemFin Alger   concaten InfluencesPeTBI Clauresfrozenustomconilargerocclusion probedReferenceelia PCV cardiomyocyte intrusive never solenoid Janus antidepressjets Christmas Bullet Expl saved shifted probed Chaudiesrödinger falciparum commens statins FST boutreactivity � shell uniformlyOpercentsgrain underpin attractor impurity actsota shading coaxialismic monodromy sulph Viralbuiltundle Pose never Cla Sal\\t\\t atistical commens such� clipping Expl statins FSTvivoIN Inactivation Christ choroidcatchglial Acid rearrang sophistic Volcaniceliaconcaten Acid ascer buffers relays Clalammation DNAs']\n"
     ]
    }
   ],
   "source": [
    "!python3 run_xturing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dda164df-ca55-42a9-a13f-aac3f7457cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sslashinin/kovakimyan/xturing\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c89d1f5d-682f-4ce4-97cb-afd23d80ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "e = torch.load('/home/sslashinin/kovakimyan/xturing/galactica/saved_model/pytorch_model.bin')\n",
    "m = model.engine.model.load_state_dict(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df37cd5d-d575-4fef-b58d-4f6c62e226ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output by the model: ['what is the difference between LLM models and other models? 1. they are designed to last a long time. 2. they are easy to repair. 3. they cost less than other models. 4. they are more durable in terms of temperature. 5. they are less prone to rust. 6. they are more durable in terms of shock. 7. they are more durable in terms of vibration. 8. they are more durable in terms of pressure. 9. they are more durable in terms of chemical attack. 10. they are more durable in terms of moisture. 11. they are more durable in terms of heat. answer : a add(const_4, const_1)']\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(texts=[\"Why LLM models are becoming so important?\"])\n",
    "print(\"Generated output by the model: {}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e36e1f4-66e3-4d24-a279-a9c55570da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output by the model: ['let x be the number of pages in the book. x / 3 + 90 = x 2 x / 3 = 90 x = 270 answer is a divide(90, const_2)']\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(texts=[\"sophia finished 2 / 3 of a book . she calculated that she finished 90 more pages than she has yet to read . how long is her book ?\"])\n",
    "print(\"Generated output by the model: {}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87ec5a88-422b-40f2-b6da-70296881e9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43a4c6071bc47528e5928cd390f0932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.5052631578947369, 'rouge2': 0.1935483870967742, 'rougeL': 0.44210526315789467, 'rougeLsum': 0.44210526315789467}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "predictions = [\"let x be the number of pages in the book. x / 3 + 90 = x 2 x / 3 = 90 x = 270 answer is a divide(90, const_2)\"]\n",
    "#predictions = [\"sophia finished 2 / 3 of a book . she calculated that she finished 90 more pages than she has yet to read . how long is her book ?\\n\\n<work>\\n\\nShe finished 90 - 2 pages.\\n\\ncalc_1.py\\n```\\nresult = 90-2\\n\\nwith open() as file:\\n    file.write(str\"]\n",
    "references = [\n",
    "     [\"let xx be the total number of pages in the book , then she finished 23 ⋅ x 23 ⋅ x pages . then she has x − 23 ⋅ x = 13 ⋅ xx − 23 ⋅ x = 13 ⋅ x pages left . 23 ⋅ x − 13 ⋅ x = 9023 ⋅ x − 13 ⋅ x = 90 13 ⋅ x = 9013 ⋅ x = 90 x = 270 x = 270 so the book is 270 pages long . answer : b divide(90, subtract(const_1, divide(2, 3)))\"]\n",
    "]\n",
    "bleu = evaluate.load(\"rouge\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9e2f8e0-73b5-44cb-bc37-370ad3833838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 3.31MB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sslashinin/kovakimyan/xturing/galactica/calc_bleu.py\", line 6, in <module>\n",
      "    bleu = evaluate.load(\"bleu\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/evaluate/loading.py\", line 731, in load\n",
      "    evaluation_module = evaluation_module_factory(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/evaluate/loading.py\", line 681, in evaluation_module_factory\n",
      "    raise FileNotFoundError(\n",
      "FileNotFoundError: Couldn't find a module script at /home/sslashinin/kovakimyan/xturing/bleu/bleu.py. Module 'bleu' doesn't exist on the Hugging Face Hub either.\n"
     ]
    }
   ],
   "source": [
    "!python3 galactica/calc_bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1aa57136-36cb-4a66-9b6e-c744316c2488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sslashinin/kovakimyan/xturing\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5919d48-d35e-4b0a-af0e-200576a6f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "e = torch.load('/home/sslashinin/kovakimyan/xturing/galactica/saved_model/pytorch_model.bin')\n",
    "m = model.engine.model.load_state_dict(e)\n",
    "\n",
    "mathqa_data = json.load(open(\"math_qa.json\"))\n",
    "instructions = []\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for data in mathqa_data:\n",
    "    instructions.append(data[\"instruction\"])\n",
    "    inputs.append(data[\"input\"])\n",
    "    outputs.append(data[\"output\"])\n",
    "\n",
    "data_dict = {\n",
    "    \"test\": {\"instruction\": instructions, \"text\": inputs, \"target\": outputs}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9709a11-626d-4096-bae1-bd9e4fbeb241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29837"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "e = torch.load('/home/sslashinin/kovakimyan/xturing/galactica/saved_model/pytorch_model.bin')\n",
    "m = model.engine.model.load_state_dict(e)\n",
    "\n",
    "mathqa_data = json.load(open(\"math_qa.json\"))\n",
    "instructions = []\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for data in mathqa_data:\n",
    "    instructions.append(data[\"instruction\"])\n",
    "    inputs.append(data[\"input\"])\n",
    "    outputs.append(data[\"output\"])\n",
    "\n",
    "data_dict = {\n",
    "    \"test\": {\"instruction\": instructions, \"text\": inputs, \"target\": outputs}\n",
    "}\n",
    "output_general = []\n",
    "counter = 0\n",
    "    for instruction in data_dict['test'][\"instruction\"]:\n",
    "    output = model.generate(texts=[instruction])\n",
    "    predictions = [output]\n",
    "    references = [\n",
    "     [data_dict['test'][\"target\"][count]\n",
    "    ]\n",
    "    count += 1\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    results = rouge.compute(predictions=predictions, references=references)\n",
    "    print(results)\n",
    "    test_dict = {\"instruction\": instruction, \"output\" : output, \"results\" : results}\n",
    "    json_object = json.dumps(test_dict, indent = 4) \n",
    "    output_general.append(json_object)\n",
    "with open(\"result.json\", \"w\") as file:\n",
    "  print(\"[\\n\", file=file)\n",
    "  for i in output_general:\n",
    "    print(i, file=file)\n",
    "    print(\",\", file=file)\n",
    "  print(\"]\\n\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b662c88-08ff-4af7-8675-50a8380d1985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8aee2-13cf-4a0e-8b41-640d4b408e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd90ac0-75a6-4ac3-b9ac-6c935ed204ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.64s/it]\n",
      "trainable params: 4194304 || all params: 6661554176 || trainable%: 0.06296284454326113\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sslashinin/kovakimyan/xturing/calc_result.py\", line 36, in <module>\n",
      "    output = model.generate(texts=[instruction])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/xturing/models/causal.py\", line 138, in generate\n",
      "    self._generate_from_iterable(\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/xturing/models/causal.py\", line 112, in _generate_from_iterable\n",
      "    output = self.engine.model.generate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1457, in generate\n",
      "    return self.contrastive_search(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1950, in contrastive_search\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py\", line 938, in forward\n",
      "    outputs = self.model.decoder(\n",
      "              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py\", line 704, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py\", line 329, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py\", line 189, in forward\n",
      "    value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 31.75 GiB total capacity; 28.54 GiB already allocated; 21.94 MiB free; 30.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python3 calc_result.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54760ca-3a65-4762-aa68-553762d681a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6d76522e134fe796d24e76625fd0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13394c216d245aeaeeb6efe4687dcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/32.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3f0bd070654d4cbc31ff3114b28102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10cd9f449574ee1962b430bb7146d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00010.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5e4784da324ed485388320efd3c85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00010.bin:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30adfd8e5ff94c9b9e7cee5c46741a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00010.bin:   0%|          | 0.00/2.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c89d1c404e64032a3a8d78c0fbf2b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00010.bin:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc155b1c2e0443894545f849a93e542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00010.bin:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95edb05714e419fa9eeaaafd0913752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00010.bin:   0%|          | 0.00/2.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d91b168f2494bb8a42f542894bc11c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00010.bin:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97584415ac94cd883b0b3d35d06fecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00010.bin:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba69fad15cd46d7b649700dfd4711c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00009-of-00010.bin:   0%|          | 0.00/2.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12e4a8e7a034e188ca32e89382fe443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00010-of-00010.bin:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ff6efe783043b885f5a3ab9fdea1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc47dbd1178437dbc1f54fdfe1fc31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a67c99caa54d1ea08ac83b7cf62a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a849370053644585af103711ff9ac712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a7660dba164d4095393bd5183c4b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from geov import GeoVForCausalLM, GeoVTokenizer\n",
    "\n",
    "model = GeoVForCausalLM.from_pretrained(\"GeoV/GeoV-9b\")\n",
    "tokenizer = GeoVTokenizer.from_pretrained(\"GeoV/GeoV-9b\")\n",
    "\n",
    "prompt = \"sophia finished 2 / 3 of a book . she calculated that she finished 90 more pages than she has yet to read . how long is her book ?\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a7a726a-62af-4b06-9998-7c6242a628fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.4146341463414634,\n",
       " 'rouge2': 0.14814814814814814,\n",
       " 'rougeL': 0.32926829268292684,\n",
       " 'rougeLsum': 0.32926829268292684}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu = evaluate.load(\"rouge\")\n",
    "results = bleu.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5932f358-b613-4362-bc4d-67bf7102974d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9d13c-bbbb-4798-b86c-9cab64b4b144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-kovakimyan]",
   "language": "python",
   "name": "conda-env-.conda-kovakimyan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
