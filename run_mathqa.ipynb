{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a930800-d06d-485f-ba4c-9c3c4dd3e79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  tokendataset_.zip\n",
      "  inflating: tokendataset_/dataset_dict.json  \n",
      "   creating: tokendataset_/test/\n",
      "  inflating: tokendataset_/test/data-00000-of-00001.arrow  \n",
      "  inflating: tokendataset_/test/dataset_info.json  \n",
      "  inflating: tokendataset_/test/state.json  \n",
      "   creating: tokendataset_/train/\n",
      "  inflating: tokendataset_/train/data-00000-of-00001.arrow  \n",
      "  inflating: tokendataset_/train/dataset_info.json  \n",
      "  inflating: tokendataset_/train/state.json  \n",
      "   creating: tokendataset_/validation/\n",
      "  inflating: tokendataset_/validation/data-00000-of-00001.arrow  \n",
      "  inflating: tokendataset_/validation/dataset_info.json  \n",
      "  inflating: tokendataset_/validation/state.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip tokendataset_.zip -d tokendataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13045aa4-9e68-4ae6-b286-ee8404712e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/galactica-125M\"\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "num_epochs = 3\n",
    "dataset = load_dataset(\"math_qa\")\n",
    "dataset = dataset.remove_columns(['options', 'correct', 'linear_formula', 'category'])\n",
    "model = OPTForCausalLM.from_pretrained(model_checkpoint)\n",
    "print(\"dataset download success!\\n\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-1.3b\", use_fast=True)\n",
    "tokenizer.pad_token_id = 1\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# setup truncation\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "# setup special tokens\n",
    "tokenizer.bos_token_id = 0\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "\n",
    "tokenizer.eos_token_id = 2\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "tokenizer.unk_token_id = 3\n",
    "SPLIT_MARKER = f\"SPL{1}T-TH{1}S-Pl3A5E\"\n",
    "def _insert_split_marker(m: re.Match):\n",
    "    \"\"\"\n",
    "    Applies split marker based on a regex match of special tokens such as\n",
    "    [START_DNA].\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : str\n",
    "        Input text to split\n",
    "    Returns\n",
    "    ----------\n",
    "    str - the text with the split token added\n",
    "    \"\"\"\n",
    "    start_token, _, sequence, end_token = m.groups()\n",
    "    sequence = re.sub(r\"(.)\", fr\"{SPLIT_MARKER}\\1\", sequence, flags=re.DOTALL)\n",
    "    return f\"{start_token}{sequence}{SPLIT_MARKER}{end_token}\"\n",
    "def escape_custom_split_sequence(text):\n",
    "    return re.compile(r\"(\\[START_(DNA|SMILES|I_SMILES|AMINO)])(.*?)(\\[END_\\2])\").sub(_insert_split_marker, text)\n",
    "def _tokenize(input_text, new_doc) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Apply custom preprocessing to input texts and tokenize them.\n",
    "    Returns\n",
    "    -------\n",
    "        input_text : list[str]\n",
    "            Texts to be tokenized\n",
    "        new_doc : bool\n",
    "            If True, prepends the end-of-document (</s>) token to each sequence and fixes\n",
    "            padding.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for text in input_text:\n",
    "        text = escape_custom_split_sequence(text)\n",
    "        if not text:\n",
    "            warnings.warn(\n",
    "                \"Found an empty input text. Changing to end-of-document token instead.\",\n",
    "                UserWarning\n",
    "            )\n",
    "            text = tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "\n",
    "    if new_doc:\n",
    "        pad_token = tokenizer.pad_token\n",
    "        texts = [pad_token + t for t in texts]\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=\"longest\",\n",
    "        max_length=500,\n",
    "        truncation=True\n",
    "    )\n",
    "    # print(encoded)\n",
    "    # context_tokens = encoded[\"input_ids\"]\n",
    "    # input_v = torch.LongTensor(context_tokens).to(model.device)\n",
    "\n",
    "    # if new_doc:\n",
    "    #     input_v[input_v[:, 0] == tokenizer.pad_token_id, 0] = tokenizer.eos_token_id\n",
    "    return encoded\n",
    "def combine_features(example):\n",
    "  problem = example['Problem']\n",
    "  rationale = example['Rationale']\n",
    "  annotated_formula = example[\"annotated_formula\"]\n",
    "  combine_feature = [f\"{problem[i]} <work> {rationale[i]} {annotated_formula[i]}\" for i in range(len(problem))]\n",
    "  return {\"text\":combine_feature}\n",
    "  #return example['Problem'] + ' <work> ' + example['Rationale'] + \" \" + example[\"annotated_formula\"]\n",
    "dataset = dataset.map(combine_features, batched=True, remove_columns=['Problem', 'Rationale', 'annotated_formula'], num_proc=4)\n",
    "\n",
    "def tokenize(example):\n",
    "  tokinized = [_tokenize(example['text'][i], False) for i in range(len(example['text']))]\n",
    "  tok_keys = tokinized[0].keys()\n",
    "  print(tok_keys)\n",
    "  result = {}\n",
    "  for i in tok_keys:\n",
    "    for j in range(len(tokinized)):\n",
    "      result[i] = result.get(i, []) + tokinized[j][i]\n",
    "  return result#_tokenize(example['text'], False)\n",
    "tokendataset = dataset.map(tokenize, batched=True, remove_columns=['text'], num_proc=4)\n",
    "tokendataset.save_to_disk(\"tokendataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28db8049-62fe-4f16-ae53-626d774babb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_from_disk(\"./tokendataset_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2c4f560-cac5-4bcd-be3e-7fb339ec338d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 13758571\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7a4e234-a272-4981-a2e4-54045a2efddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported success!\n",
      "\n",
      "DatasetDict({                                                                   \n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 101045\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10188\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 15152\n",
      "    })\n",
      "})\n",
      "dataset download success!\n",
      "\n",
      "model download success!\n",
      "\n",
      "training_args success!\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "trainer success!\n",
      "\n",
      "The following columns in the training set don't have a corresponding argument in `OPTForCausalLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `OPTForCausalLM.forward`,  you can safely ignore this message.\n",
      "/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 101045\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1185\n",
      "  Number of trainable parameters = 1315201024\n",
      "  0%|                                                  | 0/1185 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 33%|█████████████▎                          | 395/1185 [04:33<08:01,  1.64it/s]The following columns in the evaluation set don't have a corresponding argument in `OPTForCausalLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `OPTForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15152\n",
      "  Batch size = 256\n",
      "\n",
      "  0%|                                                    | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 2/60 [00:00<00:04, 12.09it/s]\u001b[A\n",
      "  7%|██▉                                         | 4/60 [00:00<00:09,  5.92it/s]\u001b[A\n",
      "  8%|███▋                                        | 5/60 [00:00<00:10,  5.35it/s]\u001b[A\n",
      " 10%|████▍                                       | 6/60 [00:01<00:12,  4.39it/s]\u001b[A\n",
      " 12%|█████▏                                      | 7/60 [00:01<00:11,  4.43it/s]\u001b[A\n",
      " 13%|█████▊                                      | 8/60 [00:01<00:12,  4.33it/s]\u001b[A\n",
      " 15%|██████▌                                     | 9/60 [00:01<00:11,  4.32it/s]\u001b[A\n",
      " 17%|███████▏                                   | 10/60 [00:02<00:11,  4.31it/s]\u001b[A\n",
      " 18%|███████▉                                   | 11/60 [00:02<00:11,  4.32it/s]\u001b[A\n",
      " 20%|████████▌                                  | 12/60 [00:02<00:11,  4.29it/s]\u001b[A\n",
      " 22%|█████████▎                                 | 13/60 [00:02<00:10,  4.30it/s]\u001b[A\n",
      " 23%|██████████                                 | 14/60 [00:03<00:10,  4.34it/s]\u001b[A\n",
      " 25%|██████████▊                                | 15/60 [00:03<00:10,  4.34it/s]\u001b[A\n",
      " 27%|███████████▍                               | 16/60 [00:03<00:10,  4.32it/s]\u001b[A\n",
      " 28%|████████████▏                              | 17/60 [00:03<00:09,  4.32it/s]\u001b[A\n",
      " 30%|████████████▉                              | 18/60 [00:03<00:09,  4.30it/s]\u001b[A\n",
      " 32%|█████████████▌                             | 19/60 [00:04<00:09,  4.30it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 20/60 [00:04<00:09,  4.33it/s]\u001b[A\n",
      " 35%|███████████████                            | 21/60 [00:04<00:09,  4.32it/s]\u001b[A\n",
      " 37%|███████████████▊                           | 22/60 [00:04<00:08,  4.32it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 23/60 [00:05<00:08,  4.31it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 24/60 [00:05<00:08,  4.29it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 25/60 [00:05<00:09,  3.86it/s]\u001b[A\n",
      " 43%|██████████████████▋                        | 26/60 [00:05<00:08,  3.99it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 27/60 [00:06<00:08,  4.08it/s]\u001b[A\n",
      " 47%|████████████████████                       | 28/60 [00:06<00:07,  4.12it/s]\u001b[A\n",
      " 48%|████████████████████▊                      | 29/60 [00:06<00:07,  4.18it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 30/60 [00:06<00:07,  4.21it/s]\u001b[A\n",
      " 52%|██████████████████████▏                    | 31/60 [00:07<00:06,  4.24it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 32/60 [00:07<00:06,  4.23it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 33/60 [00:07<00:06,  4.30it/s]\u001b[A\n",
      " 57%|████████████████████████▎                  | 34/60 [00:07<00:06,  4.28it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 35/60 [00:08<00:05,  4.28it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 36/60 [00:08<00:05,  4.32it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 37/60 [00:08<00:05,  4.31it/s]\u001b[A\n",
      " 63%|███████████████████████████▏               | 38/60 [00:08<00:05,  4.30it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 39/60 [00:08<00:04,  4.30it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 40/60 [00:09<00:04,  4.31it/s]\u001b[A\n",
      " 68%|█████████████████████████████▍             | 41/60 [00:09<00:04,  4.29it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 42/60 [00:09<00:04,  4.32it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 43/60 [00:09<00:03,  4.31it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 44/60 [00:10<00:04,  3.82it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 45/60 [00:10<00:03,  3.96it/s]\u001b[A\n",
      " 77%|████████████████████████████████▉          | 46/60 [00:10<00:03,  4.05it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 47/60 [00:10<00:03,  4.11it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 48/60 [00:11<00:02,  4.20it/s]\u001b[A\n",
      " 82%|███████████████████████████████████        | 49/60 [00:11<00:02,  4.23it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▊       | 50/60 [00:11<00:02,  4.23it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 51/60 [00:11<00:02,  4.24it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 52/60 [00:12<00:01,  4.29it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 53/60 [00:12<00:01,  4.28it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 54/60 [00:12<00:01,  4.27it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▍   | 55/60 [00:12<00:01,  4.29it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 56/60 [00:12<00:00,  4.34it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 57/60 [00:13<00:00,  4.29it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 58/60 [00:13<00:00,  4.27it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▎| 59/60 [00:13<00:00,  4.28it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': nan, 'eval_runtime': 14.3076, 'eval_samples_per_second': 1059.014, 'eval_steps_per_second': 4.194, 'epoch': 1.0}\n",
      " 33%|█████████████▎                          | 395/1185 [04:48<08:01,  1.64it/s]\n",
      "100%|███████████████████████████████████████████| 60/60 [00:14<00:00,  4.29it/s]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to try_mathqa1.3b/checkpoint-395\n",
      "Configuration saved in try_mathqa1.3b/checkpoint-395/config.json\n",
      "Model weights saved in try_mathqa1.3b/checkpoint-395/pytorch_model.bin\n",
      "tokenizer config file saved in try_mathqa1.3b/checkpoint-395/tokenizer_config.json\n",
      "Special tokens file saved in try_mathqa1.3b/checkpoint-395/special_tokens_map.json\n",
      "/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 3.6907, 'learning_rate': 0.0007181434599156119, 'epoch': 1.27}         \n",
      " 67%|██████████████████████████▋             | 790/1185 [09:39<04:01,  1.63it/s]The following columns in the evaluation set don't have a corresponding argument in `OPTForCausalLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `OPTForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15152\n",
      "  Batch size = 256\n",
      "\n",
      "  0%|                                                    | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 2/60 [00:00<00:07,  8.22it/s]\u001b[A\n",
      "  5%|██▏                                         | 3/60 [00:00<00:09,  5.95it/s]\u001b[A\n",
      "  7%|██▉                                         | 4/60 [00:00<00:10,  5.27it/s]\u001b[A\n",
      "  8%|███▋                                        | 5/60 [00:00<00:11,  4.88it/s]\u001b[A\n",
      " 10%|████▍                                       | 6/60 [00:01<00:11,  4.64it/s]\u001b[A\n",
      " 12%|█████▏                                      | 7/60 [00:01<00:11,  4.53it/s]\u001b[A\n",
      " 13%|█████▊                                      | 8/60 [00:01<00:11,  4.52it/s]\u001b[A\n",
      " 15%|██████▌                                     | 9/60 [00:01<00:11,  4.39it/s]\u001b[A\n",
      " 17%|███████▏                                   | 10/60 [00:02<00:11,  4.35it/s]\u001b[A\n",
      " 18%|███████▉                                   | 11/60 [00:02<00:11,  4.35it/s]\u001b[A\n",
      " 20%|████████▌                                  | 12/60 [00:02<00:11,  4.34it/s]\u001b[A\n",
      " 22%|█████████▎                                 | 13/60 [00:02<00:10,  4.38it/s]\u001b[A\n",
      " 23%|██████████                                 | 14/60 [00:03<00:11,  3.84it/s]\u001b[A\n",
      " 25%|██████████▊                                | 15/60 [00:03<00:11,  3.98it/s]\u001b[A\n",
      " 27%|███████████▍                               | 16/60 [00:03<00:10,  4.11it/s]\u001b[A\n",
      " 28%|████████████▏                              | 17/60 [00:03<00:10,  4.09it/s]\u001b[A\n",
      " 30%|████████████▉                              | 18/60 [00:04<00:09,  4.23it/s]\u001b[A\n",
      " 32%|█████████████▌                             | 19/60 [00:04<00:09,  4.21it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 20/60 [00:04<00:09,  4.22it/s]\u001b[A\n",
      " 35%|███████████████                            | 21/60 [00:04<00:09,  4.26it/s]\u001b[A\n",
      " 37%|███████████████▊                           | 22/60 [00:04<00:08,  4.26it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 23/60 [00:05<00:08,  4.33it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 24/60 [00:05<00:08,  4.23it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 25/60 [00:05<00:08,  4.27it/s]\u001b[A\n",
      " 43%|██████████████████▋                        | 26/60 [00:05<00:07,  4.31it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 27/60 [00:06<00:08,  3.84it/s]\u001b[A\n",
      " 47%|████████████████████                       | 28/60 [00:06<00:08,  3.98it/s]\u001b[A\n",
      " 48%|████████████████████▊                      | 29/60 [00:06<00:07,  4.08it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 30/60 [00:06<00:07,  4.12it/s]\u001b[A\n",
      " 52%|██████████████████████▏                    | 31/60 [00:07<00:06,  4.18it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 32/60 [00:07<00:06,  4.22it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 33/60 [00:07<00:06,  4.22it/s]\u001b[A\n",
      " 57%|████████████████████████▎                  | 34/60 [00:07<00:06,  4.26it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 35/60 [00:08<00:05,  4.23it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 36/60 [00:08<00:05,  4.26it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 37/60 [00:08<00:05,  4.28it/s]\u001b[A\n",
      " 63%|███████████████████████████▏               | 38/60 [00:08<00:05,  4.27it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 39/60 [00:09<00:04,  4.29it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 40/60 [00:09<00:05,  3.85it/s]\u001b[A\n",
      " 68%|█████████████████████████████▍             | 41/60 [00:09<00:04,  3.97it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 42/60 [00:09<00:04,  4.05it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 43/60 [00:10<00:04,  4.11it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 44/60 [00:10<00:03,  4.20it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 45/60 [00:10<00:03,  4.22it/s]\u001b[A\n",
      " 77%|████████████████████████████████▉          | 46/60 [00:10<00:03,  4.23it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 47/60 [00:10<00:03,  4.25it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 48/60 [00:11<00:02,  4.27it/s]\u001b[A\n",
      " 82%|███████████████████████████████████        | 49/60 [00:11<00:02,  4.28it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▊       | 50/60 [00:11<00:02,  4.33it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 51/60 [00:11<00:02,  4.32it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 52/60 [00:12<00:01,  4.29it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 53/60 [00:12<00:01,  3.80it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 54/60 [00:12<00:01,  3.94it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▍   | 55/60 [00:12<00:01,  4.04it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 56/60 [00:13<00:00,  4.09it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 57/60 [00:13<00:00,  4.16it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 58/60 [00:13<00:00,  4.18it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▎| 59/60 [00:13<00:00,  4.23it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': nan, 'eval_runtime': 14.3633, 'eval_samples_per_second': 1054.91, 'eval_steps_per_second': 4.177, 'epoch': 2.0}\n",
      " 67%|██████████████████████████▋             | 790/1185 [09:53<04:01,  1.63it/s]\n",
      "100%|███████████████████████████████████████████| 60/60 [00:14<00:00,  4.26it/s]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to try_mathqa1.3b/checkpoint-790\n",
      "Configuration saved in try_mathqa1.3b/checkpoint-790/config.json\n",
      "Model weights saved in try_mathqa1.3b/checkpoint-790/pytorch_model.bin\n",
      "tokenizer config file saved in try_mathqa1.3b/checkpoint-790/tokenizer_config.json\n",
      "Special tokens file saved in try_mathqa1.3b/checkpoint-790/special_tokens_map.json\n",
      "/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0, 'learning_rate': 0.00029620253164556966, 'epoch': 2.53}           \n",
      "100%|███████████████████████████████████████| 1185/1185 [14:31<00:00,  1.62it/s]The following columns in the evaluation set don't have a corresponding argument in `OPTForCausalLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `OPTForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15152\n",
      "  Batch size = 256\n",
      "\n",
      "  0%|                                                    | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 2/60 [00:00<00:06,  8.61it/s]\u001b[A\n",
      "  5%|██▏                                         | 3/60 [00:00<00:09,  6.02it/s]\u001b[A\n",
      "  7%|██▉                                         | 4/60 [00:00<00:10,  5.30it/s]\u001b[A\n",
      "  8%|███▋                                        | 5/60 [00:00<00:11,  4.90it/s]\u001b[A\n",
      " 10%|████▍                                       | 6/60 [00:01<00:11,  4.70it/s]\u001b[A\n",
      " 12%|█████▏                                      | 7/60 [00:01<00:11,  4.54it/s]\u001b[A\n",
      " 13%|█████▊                                      | 8/60 [00:01<00:11,  4.44it/s]\u001b[A\n",
      " 15%|██████▌                                     | 9/60 [00:01<00:11,  4.42it/s]\u001b[A\n",
      " 17%|███████▏                                   | 10/60 [00:02<00:11,  4.41it/s]\u001b[A\n",
      " 18%|███████▉                                   | 11/60 [00:02<00:11,  4.39it/s]\u001b[A\n",
      " 20%|████████▌                                  | 12/60 [00:02<00:11,  4.34it/s]\u001b[A\n",
      " 22%|█████████▎                                 | 13/60 [00:02<00:10,  4.32it/s]\u001b[A\n",
      " 23%|██████████                                 | 14/60 [00:03<00:10,  4.33it/s]\u001b[A\n",
      " 25%|██████████▊                                | 15/60 [00:03<00:10,  4.32it/s]\u001b[A\n",
      " 27%|███████████▍                               | 16/60 [00:03<00:10,  4.34it/s]\u001b[A\n",
      " 28%|████████████▏                              | 17/60 [00:03<00:09,  4.32it/s]\u001b[A\n",
      " 30%|████████████▉                              | 18/60 [00:03<00:09,  4.31it/s]\u001b[A\n",
      " 32%|█████████████▌                             | 19/60 [00:04<00:09,  4.28it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 20/60 [00:04<00:09,  4.30it/s]\u001b[A\n",
      " 35%|███████████████                            | 21/60 [00:04<00:08,  4.36it/s]\u001b[A\n",
      " 37%|███████████████▊                           | 22/60 [00:04<00:08,  4.28it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 23/60 [00:05<00:08,  4.31it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 24/60 [00:05<00:08,  4.28it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 25/60 [00:05<00:08,  4.28it/s]\u001b[A\n",
      " 43%|██████████████████▋                        | 26/60 [00:05<00:07,  4.29it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 27/60 [00:06<00:07,  4.30it/s]\u001b[A\n",
      " 47%|████████████████████                       | 28/60 [00:06<00:07,  4.29it/s]\u001b[A\n",
      " 48%|████████████████████▊                      | 29/60 [00:06<00:07,  4.28it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 30/60 [00:06<00:06,  4.29it/s]\u001b[A\n",
      " 52%|██████████████████████▏                    | 31/60 [00:06<00:06,  4.27it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 32/60 [00:07<00:06,  4.30it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 33/60 [00:07<00:06,  4.33it/s]\u001b[A\n",
      " 57%|████████████████████████▎                  | 34/60 [00:07<00:05,  4.34it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 35/60 [00:07<00:05,  4.27it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 36/60 [00:08<00:05,  4.30it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 37/60 [00:08<00:05,  4.27it/s]\u001b[A\n",
      " 63%|███████████████████████████▏               | 38/60 [00:08<00:05,  4.31it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 39/60 [00:08<00:04,  4.27it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 40/60 [00:09<00:04,  4.30it/s]\u001b[A\n",
      " 68%|█████████████████████████████▍             | 41/60 [00:09<00:04,  4.32it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 42/60 [00:09<00:04,  4.31it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 43/60 [00:09<00:03,  4.32it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 44/60 [00:09<00:03,  4.35it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 45/60 [00:10<00:03,  4.28it/s]\u001b[A\n",
      " 77%|████████████████████████████████▉          | 46/60 [00:10<00:03,  4.29it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 47/60 [00:10<00:03,  4.30it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 48/60 [00:10<00:02,  4.34it/s]\u001b[A\n",
      " 82%|███████████████████████████████████        | 49/60 [00:11<00:02,  4.27it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▊       | 50/60 [00:11<00:02,  4.27it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 51/60 [00:11<00:02,  4.26it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 52/60 [00:11<00:01,  4.30it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 53/60 [00:12<00:01,  4.31it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 54/60 [00:12<00:01,  4.29it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▍   | 55/60 [00:12<00:01,  4.25it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 56/60 [00:12<00:00,  4.29it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 57/60 [00:13<00:00,  4.31it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 58/60 [00:13<00:00,  4.28it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▎| 59/60 [00:13<00:00,  4.32it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': nan, 'eval_runtime': 13.88, 'eval_samples_per_second': 1091.645, 'eval_steps_per_second': 4.323, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████████| 1185/1185 [14:45<00:00,  1.62it/s]\n",
      "100%|███████████████████████████████████████████| 60/60 [00:13<00:00,  4.30it/s]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to try_mathqa1.3b/checkpoint-1185\n",
      "Configuration saved in try_mathqa1.3b/checkpoint-1185/config.json\n",
      "Model weights saved in try_mathqa1.3b/checkpoint-1185/pytorch_model.bin\n",
      "tokenizer config file saved in try_mathqa1.3b/checkpoint-1185/tokenizer_config.json\n",
      "Special tokens file saved in try_mathqa1.3b/checkpoint-1185/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from try_mathqa1.3b/checkpoint-395 (score: nan).\n",
      "{'train_runtime': 908.4914, 'train_samples_per_second': 333.669, 'train_steps_per_second': 1.304, 'train_loss': 1.5572773478705169, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████████| 1185/1185 [15:08<00:00,  1.30it/s]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `OPTForCausalLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `OPTForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15152\n",
      "  Batch size = 256\n",
      "/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|███████████████████████████████████████████| 60/60 [00:13<00:00,  4.29it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sslashinin/kovakimyan/run_mathqa.py\", line 114, in <module>\n",
      "    print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
      "                         ^^^^\n",
      "NameError: name 'math' is not defined\n"
     ]
    }
   ],
   "source": [
    "!python3 run_mathqa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc7a97-29c9-4018-a3d5-538e7afb3eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-kovakimyan]",
   "language": "python",
   "name": "conda-env-.conda-kovakimyan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
